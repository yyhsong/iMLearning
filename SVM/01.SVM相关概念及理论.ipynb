{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性可分和超平面\n",
    "\n",
    "### 二分类\n",
    "\n",
    "- 二分类是一切分类的基础，很多情况下多分类问题可以转化为二分类问题来解决。\n",
    "- 二分类问题就是：给定的各个样本数据分别属于两个类之一，而目标是确定新数据点将归属到哪个类中。\n",
    "\n",
    "### 特征的向量空间模型\n",
    "\n",
    "- 一个个具体的样本，在被机器学习算法处理时，由其特征来表示。\n",
    "- 即每个现实世界的事物，在用来进行机器学习训练或预测时，需要转化为一个特征向量。\n",
    "- 假设样本的特征向量为n维，那么这些样本的特征向量处在n维的特征空间中。\n",
    "- 特征空间可以是欧式空间，也可以是希尔伯特空间。\n",
    "\n",
    "### 线性可分\n",
    "\n",
    "- 选取特征的目的：将一个事物的某些属性数字化，再映射为特征空间中的点，其目的当然是为了对其进行计算。\n",
    "\n",
    "### 超平面\n",
    "\n",
    "- 超平面：n维欧式空间中维度等于n-1的线性子空间。\n",
    "- 将线性可分的样本用超平面分隔开的分类模型，叫做线性分类模型，或线性分类器。\n",
    "- 以最大间隔把两类样本分开的超平面，是最佳超平面，也叫最大间隔超平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性可分支持向量机 \n",
    "\n",
    "- 支持向量机 SVM: Support Vector Machine\n",
    "- 线性可分支持向量机就是以找出线性可分的样本在特征空间中的最大间隔超平面为学习目的的分类模型。\n",
    "- SVM的学习目标求解最大间隔超平面问题，其实是一个约束条件下的最优化问题。\n",
    "- 该最优化问题可以使用拉格朗日乘子法进行求解，而不能使用梯度下降法，因为梯度下降法是没有约束条件问题的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拉格朗日函数\n",
    "\n",
    "- 函数的梯度：表示该函数在某点处的方向导数，方向导数是某个多维函数上的点沿每个维度分别求导后，再组合而成的向量。\n",
    "- 一个函数的梯度与它的等高线垂直。\n",
    "- 拉格朗日函数把原本的目标函数和其限制条件整合成了一个函数。\n",
    "- 于是原本有约束的优化问题，就可以转化为对拉格朗日函数的无约束优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性SVM，从硬间隔到软间隔\n",
    "\n",
    "### 线性可分SVM\n",
    "- 线性可分SVM成立的前提是训练样本在向量空间中线性可分，即存在一个超平面能够将不同类的样本完全彻底且无一错漏地分开。此时两个辅助超平面之间的间隔叫做硬间隔（Hard Margin），里面是没有任何样本存在的。\n",
    "\n",
    "### 线性SVM\n",
    "- 由于样本线性可分的情况在现实当中出现很少，为了更有效地应对实际问题，不再要求所有不同类的样本全部线性可分，也就是不再要求硬间隔存在。取而代之的是将不同类样本之间的硬间隔变成软间隔，即允许部分样本不满足约束条件。\n",
    "- 对于线性 SVM而言，除了落在两个辅助超平面上的样本，落在软间隔之内的样本也是它的支持向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非线性SVM和核函数\n",
    "\n",
    "### 非线性分类问题\n",
    "- 遇到分类问题的时候，最理想的状态，当然是样本在向量空间中都是线性可分的，可以清晰无误地把它们分隔成不同的类别——线性可分 SVM。\n",
    "- 如果实在不行，可以容忍少数不能被正确划分，只要大多数线性可分就好——线性 SVM。\n",
    "- 解决非线性分类问题的基本思路，让在二维空间中线性不可分的样本，在更高维度的空间里线性可分。\n",
    "\n",
    "### 非线性SVM\n",
    "- 对于在有限维度向量空间中线性不可分的样本，将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。\n",
    "- 将样本映射到的这个更高维度的新空间叫做特征空间。\n",
    "- 非线性 SVM = 核技巧 + 线性 SVM。\n",
    "\n",
    "### 核函数的种类\n",
    "- 线性核 Linear Kernel\n",
    "- 多项式核 Polynomial Kernel\n",
    "- RBF核 Radial Basis Function Kernel\n",
    "- Sigmoid核 Sigmoid Kernel\n",
    "\n",
    "### 数据归一化 Data Normalization\n",
    "- 数据归一化是一种数据处理方法，具体所做的就是对取值范畴不同的数据进行归一化处理，使它们处在同一数量级上。\n",
    "- 最常见的，就是把各种数据都变成(0, 1)之间的小数。\n",
    "- 常用的归一化算法有：线性转换、标准分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量回归（SVR: Support Vector Regression）\n",
    "\n",
    "- 支持向量回归模型的模型函数也是一个线性函数： y = wx + b。\n",
    "- SVR 和线性回归的模型函数看起来一样，但却是两个不同的回归模型：计算损失的原则不同，目标函数和最优化算法也不同。\n",
    "- SVR 在线性函数两侧制造了一个“间隔带”，对于所有落入到间隔带内的样本，都不计算损失；只有间隔带之外的，才计入损失函数。之后再通过最小化间隔带的宽度与总损失来最优化模型。\n",
    "- 有一点 SVR 和 SVM 正相反，那就是：SVR 期望所有的样本点都落在“隔离带”里面，而 SVM 则恰恰希望所有的样本点都在“隔离带”之外！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
